{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97baa430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937cad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['price day ahead', 'price actual'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Carrega o dataset completo com preços, geração, load, e clima\n",
    "df = pd.read_csv(\"dataset.csv\", parse_dates=['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "# Verifica se a coluna de preço está presente\n",
    "print(df.columns[df.columns.str.contains(\"price\", case=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd49b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_and_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['lag_1h'] = df['price actual'].shift(1)\n",
    "    df['lag_24h'] = df['price actual'].shift(24)\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    return df.dropna()\n",
    "\n",
    "df_prepared = add_lag_and_time_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf4d48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janela 1:\n",
      "  Treino: 2015-01-01 → 2015-12-31  (8737 registos)\n",
      "  Teste:  2016-01-01 → 2016-12-31  (8784 registos)\n",
      "\n",
      "Janela 2:\n",
      "  Treino: 2015-01-01 → 2016-12-31  (17521 registos)\n",
      "  Teste:  2017-01-01 → 2017-12-31  (8760 registos)\n",
      "\n",
      "Janela 3:\n",
      "  Treino: 2015-01-01 → 2017-12-31  (26281 registos)\n",
      "  Teste:  2018-01-01 → 2018-12-31  (8759 registos)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_custom_sliding_windows(df):\n",
    "    \"\"\"\n",
    "    Cria janelas fixas com:\n",
    "    - Janela 1: treino 2015, teste 2016\n",
    "    - Janela 2: treino 2015-2016, teste 2017\n",
    "    - Janela 3: treino 2015–2017, teste 2018\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_index()\n",
    "\n",
    "    splits = []\n",
    "\n",
    "    configs = [\n",
    "        ('2015-01-01', '2015-12-31', '2016-01-01', '2016-12-31'),  # Janela 1\n",
    "        ('2015-01-01', '2016-12-31', '2017-01-01', '2017-12-31'),  # Janela 2\n",
    "        ('2015-01-01', '2017-12-31', '2018-01-01', '2018-12-31'),  # Janela 3\n",
    "    ]\n",
    "\n",
    "    for train_start, train_end, test_start, test_end in configs:\n",
    "        df_train = df.loc[train_start:train_end]\n",
    "        df_test = df.loc[test_start:test_end]\n",
    "        splits.append((df_train, df_test))\n",
    "\n",
    "    return splits\n",
    "\n",
    "# Criar os splits\n",
    "splits = create_custom_sliding_windows(df_prepared)\n",
    "\n",
    "# Visualizar os períodos\n",
    "for i, (train, test) in enumerate(splits, start=1):\n",
    "    print(f\"Janela {i}:\")\n",
    "    print(f\"  Treino: {train.index.min().date()} → {train.index.max().date()}  ({len(train)} registos)\")\n",
    "    print(f\"  Teste:  {test.index.min().date()} → {test.index.max().date()}  ({len(test)} registos)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0a96cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 168\n",
    "df_prepared['target'] = df_prepared['price actual'].shift(-forecast_horizon)\n",
    "df_prepared = df_prepared.dropna(subset=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f18eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def longterm_sliding_expanding_linear_regression(df, train_start, train_end, test_start, test_end, forecast_horizon=168):\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    \n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    # Hiperparâmetros (ajustáveis)\n",
    "    fit_intercept = True\n",
    "    normalize = False  # já usamos StandardScaler\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    total_steps = ((final_test_end - current_test_start).days + 1) // 7\n",
    "    checkpoints = {int(total_steps * 0.25): \"25%\", int(total_steps * 0.5): \"50%\", int(total_steps * 0.75): \"75%\"}\n",
    "\n",
    "    step = 0\n",
    "    while current_test_start + pd.Timedelta(hours=forecast_horizon - 1) <= final_test_end:\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        drop_cols = ['price actual', 'price day ahead']\n",
    "        feature_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "        X_train = df_train[feature_cols].values\n",
    "        y_train = df_train['price actual'].values\n",
    "        X_test = df_test[feature_cols].values\n",
    "        y_test = df_test['price actual'].values\n",
    "\n",
    "        # Escalamento\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Modelo\n",
    "        model = LinearRegression(fit_intercept=fit_intercept)\n",
    "        model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "        y_pred_scaled = model.predict(X_test_scaled)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Métricas\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "        rmae = mae / np.mean(np.abs(y_test))\n",
    "\n",
    "        results.append({\n",
    "            'start': current_test_start,\n",
    "            'end': current_test_end,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'rMAE': rmae\n",
    "        })\n",
    "\n",
    "        # Avançar + contar progresso\n",
    "        current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "        step += 1\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056f7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "📊 Métricas médias - Linear Regression (Window 2):\n",
      "MAE          8.9377\n",
      "RMSE        10.4037\n",
      "MAPE (%)    22.5861\n",
      "rMAE         0.2000\n",
      "dtype: float64\n",
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "📊 Métricas médias - Linear Regression (Window 2):\n",
      "MAE          8.0674\n",
      "RMSE         9.8431\n",
      "MAPE (%)    13.3984\n",
      "rMAE         0.1316\n",
      "dtype: float64\n",
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "📊 Métricas médias - Linear Regression (Window 2):\n",
      "MAE          8.6435\n",
      "RMSE        10.0758\n",
      "MAPE (%)    14.9051\n",
      "rMAE         0.1378\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_lr_window1 = longterm_sliding_expanding_linear_regression(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2015-12-31',\n",
    "    test_start='2016-01-01',\n",
    "    test_end='2016-12-31'\n",
    ")\n",
    "\n",
    "mean_metrics = results_lr_window1[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean()\n",
    "print(\"📊 Métricas médias - Linear Regression (Window 2):\")\n",
    "print(mean_metrics.round(4))\n",
    "\n",
    "results_lr_window2 = longterm_sliding_expanding_linear_regression(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-12-31'\n",
    ")\n",
    "\n",
    "mean_metrics = results_lr_window2[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean()\n",
    "print(\"📊 Métricas médias - Linear Regression (Window 2):\")\n",
    "print(mean_metrics.round(4))\n",
    "\n",
    "results_lr_window3 = longterm_sliding_expanding_linear_regression(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2017-12-31',\n",
    "    test_start='2018-01-01',\n",
    "    test_end='2018-12-31'\n",
    ")\n",
    "\n",
    "mean_metrics = results_lr_window3[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean()\n",
    "print(\"📊 Métricas médias - Linear Regression (Window 2):\")\n",
    "print(mean_metrics.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebb456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def longterm_sliding_expanding_rf(\n",
    "    df,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    test_start,\n",
    "    test_end,\n",
    "    forecast_horizon=168,\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    "):\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    \n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    results = []\n",
    "    total_steps = ((final_test_end - current_test_start).days + 1) // 7\n",
    "    checkpoints = {int(total_steps * 0.25): \"25%\", int(total_steps * 0.5): \"50%\", int(total_steps * 0.75): \"75%\"}\n",
    "\n",
    "    step = 0\n",
    "    while current_test_start + pd.Timedelta(hours=forecast_horizon - 1) <= final_test_end:\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        drop_cols = ['price actual', 'price day ahead']\n",
    "        feature_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "        X_train = df_train[feature_cols].values\n",
    "        y_train = df_train['price actual'].values\n",
    "        X_test = df_test[feature_cols].values\n",
    "        y_test = df_test['price actual'].values\n",
    "\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "        y_pred_scaled = model.predict(X_test_scaled)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "        rmae = mae / np.mean(np.abs(y_test))\n",
    "\n",
    "        results.append({\n",
    "            'start': current_test_start,\n",
    "            'end': current_test_end,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'rMAE': rmae\n",
    "        })\n",
    "\n",
    "        current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "        step += 1\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1087da8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE          8.7986\n",
      "RMSE        10.4056\n",
      "MAPE (%)    22.0690\n",
      "rMAE         0.1946\n",
      "dtype: float64\n",
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE          6.7078\n",
      "RMSE         8.2744\n",
      "MAPE (%)    11.1213\n",
      "rMAE         0.1090\n",
      "dtype: float64\n",
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE          7.9329\n",
      "RMSE         9.3527\n",
      "MAPE (%)    13.9505\n",
      "rMAE         0.1262\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rf_results_window1 = longterm_sliding_expanding_rf(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2015-12-31',\n",
    "    test_start='2016-01-01',\n",
    "    test_end='2016-12-31'\n",
    ")\n",
    "\n",
    "print(rf_results_window1[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n",
    "\n",
    "rf_results_window2 = longterm_sliding_expanding_rf(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-12-31'\n",
    ")\n",
    "\n",
    "print(rf_results_window2[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n",
    "\n",
    "rf_results_window3 = longterm_sliding_expanding_rf(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2017-12-31',\n",
    "    test_start='2018-01-01',\n",
    "    test_end='2018-12-31'\n",
    ")\n",
    "\n",
    "print(rf_results_window3[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fedb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def longterm_sliding_expanding_xgb(\n",
    "    df,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    test_start,\n",
    "    test_end,\n",
    "    forecast_horizon=168,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    gamma=0,\n",
    "    random_state=42\n",
    "):\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    \n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    results = []\n",
    "    total_steps = ((final_test_end - current_test_start).days + 1) // 7\n",
    "    checkpoints = {int(total_steps * 0.25): \"25%\", int(total_steps * 0.5): \"50%\", int(total_steps * 0.75): \"75%\"}\n",
    "\n",
    "    step = 0\n",
    "    while current_test_start + pd.Timedelta(hours=forecast_horizon - 1) <= final_test_end:\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        drop_cols = ['price actual', 'price day ahead']\n",
    "        feature_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "        X_train = df_train[feature_cols].values\n",
    "        y_train = df_train['price actual'].values\n",
    "        X_test = df_test[feature_cols].values\n",
    "        y_test = df_test['price actual'].values\n",
    "\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            gamma=gamma,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "        y_pred_scaled = model.predict(X_test_scaled)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "        rmae = mae / np.mean(np.abs(y_test))\n",
    "\n",
    "        results.append({\n",
    "            'start': current_test_start,\n",
    "            'end': current_test_end,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'rMAE': rmae\n",
    "        })\n",
    "\n",
    "        current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "        step += 1\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2730bed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE          8.1308\n",
      "RMSE         9.6235\n",
      "MAPE (%)    20.3367\n",
      "rMAE         0.1804\n",
      "dtype: float64\n",
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE          6.4525\n",
      "RMSE         7.9684\n",
      "MAPE (%)    10.8006\n",
      "rMAE         0.1054\n",
      "dtype: float64\n",
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE          7.7427\n",
      "RMSE         9.0741\n",
      "MAPE (%)    13.6231\n",
      "rMAE         0.1235\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rf_results_window1 = longterm_sliding_expanding_xgb(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2015-12-31',\n",
    "    test_start='2016-01-01',\n",
    "    test_end='2016-12-31'\n",
    ")\n",
    "\n",
    "print(rf_results_window1[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n",
    "\n",
    "rf_results_window2 = longterm_sliding_expanding_xgb(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-12-31'\n",
    ")\n",
    "\n",
    "print(rf_results_window2[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n",
    "\n",
    "rf_results_window3 = longterm_sliding_expanding_xgb(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2017-12-31',\n",
    "    test_start='2018-01-01',\n",
    "    test_end='2018-12-31'\n",
    ")\n",
    "\n",
    "print(rf_results_window3[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longterm_sliding_expanding_rnn(\n",
    "    df,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    test_start,\n",
    "    test_end,\n",
    "    forecast_horizon=168,\n",
    "    timesteps=24,\n",
    "    units=50,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    activation='tanh'\n",
    "):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import SimpleRNN, Dense, Input\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "\n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    results = []\n",
    "    total_steps = ((final_test_end - current_test_start).days + 1) // 7\n",
    "    checkpoints = {int(total_steps * 0.25): \"25%\", int(total_steps * 0.5): \"50%\", int(total_steps * 0.75): \"75%\"}\n",
    "\n",
    "    step = 0\n",
    "    while current_test_start + pd.Timedelta(hours=forecast_horizon - 1) <= final_test_end:\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        drop_cols = ['price actual', 'price day ahead']\n",
    "        feature_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "        if len(df_test) < forecast_horizon or len(df_train) < timesteps + 1:\n",
    "            print(f\"⚠️ Ignorado (dados insuficientes): {current_test_start.date()}\")\n",
    "            current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "            step += 1\n",
    "            continue\n",
    "\n",
    "        X_train = df_train[feature_cols].values\n",
    "        y_train = df_train['price actual'].values\n",
    "\n",
    "        y_train_shifted = y_train[timesteps:]\n",
    "        X_train = X_train[:-timesteps]\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train_shifted.reshape(-1, 1)).ravel()\n",
    "\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(timesteps, len(X_train_scaled)):\n",
    "            X_seq.append(X_train_scaled[i - timesteps:i])\n",
    "            y_seq.append(y_train_scaled[i])\n",
    "        X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X_seq.shape[1], X_seq.shape[2])))\n",
    "        model.add(SimpleRNN(units=units, activation=activation))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_seq, y_seq, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(forecast_horizon):\n",
    "            if i + timesteps > len(df_test):\n",
    "                break\n",
    "            input_seq = df_test[feature_cols].iloc[i:i + timesteps].values\n",
    "            input_seq_scaled = scaler_X.transform(input_seq).reshape((1, timesteps, len(feature_cols)))\n",
    "            pred_scaled = model.predict(input_seq_scaled, verbose=0)[0][0]\n",
    "            pred = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
    "            preds.append(pred)\n",
    "\n",
    "        y_test = df_test['price actual'].iloc[timesteps:timesteps + len(preds)].values\n",
    "        aligned_len = min(len(preds), len(y_test))\n",
    "        if aligned_len == 0:\n",
    "            print(f\"⚠️ Ignorado (sem dados válidos): {current_test_start.date()}\")\n",
    "            current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "            step += 1\n",
    "            continue\n",
    "\n",
    "        mae = mean_absolute_error(y_test[:aligned_len], preds[:aligned_len])\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[:aligned_len], preds[:aligned_len]))\n",
    "        mape = mean_absolute_percentage_error(y_test[:aligned_len], preds[:aligned_len]) * 100\n",
    "        rmae = mae / np.mean(np.abs(y_test[:aligned_len]))\n",
    "\n",
    "        results.append({\n",
    "            'start': current_test_start,\n",
    "            'end': current_test_end,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'rMAE': rmae\n",
    "        })\n",
    "\n",
    "        current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "        step += 1\n",
    "\n",
    "    if not results:\n",
    "        print(\"⚠️ Nenhuma previsão válida foi registada.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results_window2 = longterm_sliding_expanding_rnn(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-12-31'\n",
    ")\n",
    "\n",
    "print(rnn_results_window2[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results_window1 = longterm_sliding_expanding_rnn(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2015-12-31',\n",
    "    test_start='2016-01-01',\n",
    "    test_end='2016-12-31'\n",
    ")\n",
    "\n",
    "print(rnn_results_window1[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n",
    "\n",
    "rnn_results_window3 = longterm_sliding_expanding_rnn(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-12-31'\n",
    ")\n",
    "\n",
    "print(rnn_results_window3[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n",
    "\n",
    "rnn_results_window4 = longterm_sliding_expanding_rnn(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2017-12-31',\n",
    "    test_start='2018-01-01',\n",
    "    test_end='2018-12-31'\n",
    ")\n",
    "\n",
    "print(rnn_results_window4[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c351bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 A correr janela 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Progresso: 50% da janela de teste\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 A correr janela 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Progresso: 50% da janela de teste\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 A correr janela 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Erro ao ajustar o modelo SARIMA no passo 3: Unable to allocate 523. MiB for an array with shape (51, 51, 26353) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Erro ao ajustar o modelo SARIMA no passo 6: Unable to allocate 524. MiB for an array with shape (51, 51, 26425) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Erro ao ajustar o modelo SARIMA no passo 9: Unable to allocate 526. MiB for an array with shape (51, 51, 26497) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Erro ao ajustar o modelo SARIMA no passo 12: Unable to allocate 527. MiB for an array with shape (51, 51, 26569) and data type float64\n",
      "⏳ Progresso: 50% da janela de teste\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Erro ao ajustar o modelo SARIMA no passo 15: Unable to allocate 529. MiB for an array with shape (51, 51, 26641) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Erro ao ajustar o modelo SARIMA no passo 18: Unable to allocate 530. MiB for an array with shape (51, 51, 26713) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Erro ao ajustar o modelo SARIMA no passo 21: Unable to allocate 532. MiB for an array with shape (51, 51, 26785) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Erro ao ajustar o modelo SARIMA no passo 24: Unable to allocate 533. MiB for an array with shape (51, 51, 26857) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Erro ao ajustar o modelo SARIMA no passo 27: Unable to allocate 534. MiB for an array with shape (51, 51, 26929) and data type float64\n",
      "\n",
      "📊 Resultados Finais SARIMA (baseline otimizado):\n",
      "        MAE       RMSE   MAPE (%)      rMAE    janela\n",
      "0  9.133208  12.321508  22.147939  0.199098  Janela 1\n",
      "1  6.946431   8.610829   9.149975  0.089603  Janela 2\n",
      "2  7.203733   8.304807  38.790888  0.336571  Janela 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# Versão otimizada da função SARIMA\n",
    "def sliding_expanding_longterm_sarima(\n",
    "    df,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    test_start,\n",
    "    test_end,\n",
    "    forecast_horizon=168,\n",
    "    order=(1, 1, 1),\n",
    "    seasonal_order=(1, 1, 1, 24),\n",
    "    step_size_days=7  # prever de 3 em 3 dias\n",
    "):\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "\n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    steps = (final_test_end - current_test_start).days + 1\n",
    "    checkpoints = {int(steps * 0.25): \"25%\", int(steps * 0.5): \"50%\", int(steps * 0.75): \"75%\"}\n",
    "\n",
    "    for step in range(0, steps, step_size_days):  # previsão intercalada\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        current_test_start = pd.to_datetime(test_start) + pd.Timedelta(days=step)\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "\n",
    "        if current_test_end > final_test_end:\n",
    "            break\n",
    "\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        if len(df_test) < forecast_horizon:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            model = SARIMAX(\n",
    "                df_train['price actual'],\n",
    "                order=order,\n",
    "                seasonal_order=seasonal_order,\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False\n",
    "            )\n",
    "            model_fit = model.fit(disp=False)\n",
    "            forecast = model_fit.forecast(steps=forecast_horizon)\n",
    "            y_true = df_test['price actual'].values\n",
    "            y_pred = forecast.values\n",
    "\n",
    "            all_y_true.extend(y_true)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erro ao ajustar o modelo SARIMA no passo {step}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Métricas finais\n",
    "    if len(all_y_true) > 0:\n",
    "        mae = mean_absolute_error(all_y_true, all_y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(all_y_true, all_y_pred))\n",
    "        mape = mean_absolute_percentage_error(all_y_true, all_y_pred) * 100\n",
    "        rmae = mae / np.mean(np.abs(all_y_true))\n",
    "\n",
    "        return pd.DataFrame([{\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'rMAE': rmae\n",
    "        }])\n",
    "    else:\n",
    "        print(\"⚠️ Não foram feitas previsões suficientes.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Define as 3 janelas com teste reduzido a janeiro\n",
    "windows = [\n",
    "    {'train_start': '2015-01-01', 'train_end': '2015-12-31', 'test_start': '2016-01-01', 'test_end': '2016-01-31'},\n",
    "    {'train_start': '2015-01-01', 'train_end': '2016-12-31', 'test_start': '2017-01-01', 'test_end': '2017-01-31'},\n",
    "    {'train_start': '2015-01-01', 'train_end': '2017-12-31', 'test_start': '2018-01-01', 'test_end': '2018-01-31'},\n",
    "]\n",
    "\n",
    "# Guardar os resultados\n",
    "results_list = []\n",
    "\n",
    "for i, w in enumerate(windows):\n",
    "    print(f\"\\n🚀 A correr janela {i+1}\")\n",
    "    result = sliding_expanding_longterm_sarima(\n",
    "        df=df_prepared,\n",
    "        train_start=w['train_start'],\n",
    "        train_end=w['train_end'],\n",
    "        test_start=w['test_start'],\n",
    "        test_end=w['test_end'],\n",
    "        forecast_horizon=24,\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(1, 1, 1, 24),\n",
    "        step_size_days=3\n",
    "    )\n",
    "    result['janela'] = f\"Janela {i+1}\"\n",
    "    results_list.append(result)\n",
    "\n",
    "# Mostrar tabela final\n",
    "results_df = pd.concat(results_list, ignore_index=True)\n",
    "print(\"\\n📊 Resultados Finais SARIMA (baseline otimizado):\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "810cf070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longterm_sliding_expanding_lstm(\n",
    "    df,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    test_start,\n",
    "    test_end,\n",
    "    forecast_horizon=168,\n",
    "    timesteps=24,\n",
    "    units=50,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    activation='relu'\n",
    "):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "\n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    results = []\n",
    "    total_steps = ((final_test_end - current_test_start).days + 1) // 7\n",
    "    checkpoints = {int(total_steps * 0.25): \"25%\", int(total_steps * 0.5): \"50%\", int(total_steps * 0.75): \"75%\"}\n",
    "\n",
    "    step = 0\n",
    "    while current_test_start + pd.Timedelta(hours=forecast_horizon - 1) <= final_test_end:\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        drop_cols = ['price actual', 'price day ahead']\n",
    "        feature_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "        if len(df_test) < forecast_horizon or len(df_train) < timesteps + 1:\n",
    "            current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "            step += 1\n",
    "            continue\n",
    "\n",
    "        X_train = df_train[feature_cols].values\n",
    "        y_train = df_train['price actual'].values\n",
    "\n",
    "        y_train_shifted = y_train[timesteps:]\n",
    "        X_train = X_train[:-timesteps]\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train_shifted.reshape(-1, 1)).ravel()\n",
    "\n",
    "        # Criar sequências\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(timesteps, len(X_train_scaled)):\n",
    "            X_seq.append(X_train_scaled[i - timesteps:i])\n",
    "            y_seq.append(y_train_scaled[i])\n",
    "        X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "        # Modelo LSTM\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X_seq.shape[1], X_seq.shape[2])))\n",
    "        model.add(LSTM(units=units, activation=activation))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_seq, y_seq, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Previsão sequencial\n",
    "        preds = []\n",
    "        for i in range(forecast_horizon):\n",
    "            if i + timesteps > len(df_test):\n",
    "                break\n",
    "            input_seq = df_test[feature_cols].iloc[i:i + timesteps].values\n",
    "            input_seq_scaled = scaler_X.transform(input_seq).reshape((1, timesteps, len(feature_cols)))\n",
    "            pred_scaled = model.predict(input_seq_scaled, verbose=0)[0][0]\n",
    "            pred = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
    "            preds.append(pred)\n",
    "\n",
    "        y_test = df_test['price actual'].iloc[timesteps:timesteps + len(preds)].values\n",
    "        aligned_len = min(len(preds), len(y_test))\n",
    "        if aligned_len == 0:\n",
    "            current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "            step += 1\n",
    "            continue\n",
    "\n",
    "        mae = mean_absolute_error(y_test[:aligned_len], preds[:aligned_len])\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[:aligned_len], preds[:aligned_len]))\n",
    "        mape = mean_absolute_percentage_error(y_test[:aligned_len], preds[:aligned_len]) * 100\n",
    "        rmae = mae / np.mean(np.abs(y_test[:aligned_len]))\n",
    "\n",
    "        results.append({\n",
    "            'start': current_test_start,\n",
    "            'end': current_test_end,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'rMAE': rmae\n",
    "        })\n",
    "\n",
    "        current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "        step += 1\n",
    "\n",
    "    if not results:\n",
    "        print(\"⚠️ Nenhuma previsão válida foi registada.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f314134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE         10.7700\n",
      "RMSE        12.7511\n",
      "MAPE (%)    28.7059\n",
      "rMAE         0.2470\n",
      "dtype: float64\n",
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE          7.4195\n",
      "RMSE         8.9001\n",
      "MAPE (%)    11.9775\n",
      "rMAE         0.1188\n",
      "dtype: float64\n",
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE          9.0254\n",
      "RMSE        10.6575\n",
      "MAPE (%)    16.6034\n",
      "rMAE         0.1475\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "lstm_results_window1 = longterm_sliding_expanding_lstm(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2015-12-31',\n",
    "    test_start='2016-01-01',\n",
    "    test_end='2016-12-31'\n",
    ")\n",
    "\n",
    "print(lstm_results_window1[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n",
    "\n",
    "lstm_results_window2 = longterm_sliding_expanding_lstm(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-12-31'\n",
    ")\n",
    "\n",
    "print(lstm_results_window2[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))\n",
    "\n",
    "lstm_results_window3 = longterm_sliding_expanding_lstm(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2017-12-31',\n",
    "    test_start='2018-01-01',\n",
    "    test_end='2018-12-31'\n",
    ")\n",
    "\n",
    "print(lstm_results_window3[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a9a56",
   "metadata": {},
   "source": [
    "# XGBoost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd63637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def longterm_sliding_expanding_tunning_xgb(\n",
    "    df,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    test_start,\n",
    "    test_end,\n",
    "    forecast_horizon=168,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    gamma=0,\n",
    "    random_state=42\n",
    "):\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    \n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    results = []\n",
    "    total_steps = ((final_test_end - current_test_start).days + 1) // 7\n",
    "    checkpoints = {int(total_steps * 0.25): \"25%\", int(total_steps * 0.5): \"50%\", int(total_steps * 0.75): \"75%\"}\n",
    "\n",
    "    step = 0\n",
    "    while current_test_start + pd.Timedelta(hours=forecast_horizon - 1) <= final_test_end:\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        drop_cols = ['price actual', 'price day ahead']\n",
    "        feature_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "        X_train = df_train[feature_cols].values\n",
    "        y_train = df_train['price actual'].values\n",
    "        X_test = df_test[feature_cols].values\n",
    "        y_test = df_test['price actual'].values\n",
    "\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            gamma=gamma,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "        y_pred_scaled = model.predict(X_test_scaled)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "        rmae = mae / np.mean(np.abs(y_test))\n",
    "\n",
    "        results.append({\n",
    "            'start': current_test_start,\n",
    "            'end': current_test_end,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'rMAE': rmae\n",
    "        })\n",
    "\n",
    "        current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "        step += 1\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c0594c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "MAE          6.4428\n",
      "RMSE         7.9837\n",
      "MAPE (%)    10.7500\n",
      "rMAE         0.1051\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "xgb_tunning_results_window2 = longterm_sliding_expanding_tunning_xgb(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-12-31'\n",
    ")\n",
    "\n",
    "print(xgb_tunning_results_window2[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ae7e8",
   "metadata": {},
   "source": [
    "# Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0554d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def longterm_sliding_expanding_rf_tunning(\n",
    "    df,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    test_start,\n",
    "    test_end,\n",
    "    forecast_horizon=168,\n",
    "    n_estimators=300,\n",
    "    max_depth=20,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    "):\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    \n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    results = []\n",
    "    total_steps = ((final_test_end - current_test_start).days + 1) // 7\n",
    "    checkpoints = {int(total_steps * 0.25): \"25%\", int(total_steps * 0.5): \"50%\", int(total_steps * 0.75): \"75%\"}\n",
    "\n",
    "    step = 0\n",
    "    while current_test_start + pd.Timedelta(hours=forecast_horizon - 1) <= final_test_end:\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        drop_cols = ['price actual', 'price day ahead']\n",
    "        feature_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "        X_train = df_train[feature_cols].values\n",
    "        y_train = df_train['price actual'].values\n",
    "        X_test = df_test[feature_cols].values\n",
    "        y_test = df_test['price actual'].values\n",
    "\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "        y_pred_scaled = model.predict(X_test_scaled)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "        rmae = mae / np.mean(np.abs(y_test))\n",
    "\n",
    "        results.append({\n",
    "            'start': current_test_start,\n",
    "            'end': current_test_end,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'rMAE': rmae\n",
    "        })\n",
    "\n",
    "        current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "        step += 1\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4ff97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Progresso: 25% da janela de teste\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "⏳ Progresso: 75% da janela de teste\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rf_results_tunning_window2 \u001b[38;5;241m=\u001b[39m \u001b[43mlongterm_sliding_expanding_rf_tunning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2015-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2016-12-31\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2017-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2017-12-31\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(rf_results_tunning_window2[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAPE (\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrMAE\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[1;32mIn[42], line 62\u001b[0m, in \u001b[0;36mlongterm_sliding_expanding_rf_tunning\u001b[1;34m(df, train_start, train_end, test_start, test_end, forecast_horizon, n_estimators, max_depth, min_samples_split, min_samples_leaf, random_state)\u001b[0m\n\u001b[0;32m     52\u001b[0m y_train_scaled \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39mfit_transform(y_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\n\u001b[0;32m     55\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[0;32m     56\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     61\u001b[0m )\n\u001b[1;32m---> 62\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m y_pred_scaled \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n\u001b[0;32m     65\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_scaled\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf_results_tunning_window2 = longterm_sliding_expanding_rf_tunning(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-12-31'\n",
    ")\n",
    "\n",
    "print(rf_results_tunning_window2[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611dee4",
   "metadata": {},
   "source": [
    "# LSTM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e883109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longterm_sliding_expanding_lstm_tuning(\n",
    "    df,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    test_start,\n",
    "    test_end,\n",
    "    forecast_horizon=168,\n",
    "    timesteps=168,\n",
    "    units=50,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    activation='relu',\n",
    "    learning_rate=0.001,\n",
    "    dropout=0.0,\n",
    "    num_layers=1\n",
    "):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "\n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    results = []\n",
    "    total_steps = ((final_test_end - current_test_start).days + 1) // 7\n",
    "    checkpoints = {int(total_steps * 0.25): \"25%\", int(total_steps * 0.5): \"50%\", int(total_steps * 0.75): \"75%\"}\n",
    "\n",
    "    step = 0\n",
    "    while current_test_start + pd.Timedelta(hours=forecast_horizon - 1) <= final_test_end:\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        drop_cols = ['price actual', 'price day ahead']\n",
    "        feature_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "        print(f\"📅 [{step+1}] Janela: {current_test_start.date()} → {current_test_end.date()} | Train: {len(df_train)}, Test: {len(df_test)}\")\n",
    "\n",
    "        if len(df_test) < forecast_horizon or len(df_train) < timesteps + 1:\n",
    "            print(\"⚠️ Janela ignorada (dados insuficientes)\")\n",
    "            current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "            step += 1\n",
    "            continue\n",
    "\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        X_train = df_train[feature_cols].values\n",
    "        y_train = df_train['price actual'].values\n",
    "\n",
    "        y_train_shifted = y_train[timesteps:]\n",
    "        X_train = X_train[:-timesteps]\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train_shifted.reshape(-1, 1)).ravel()\n",
    "\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(timesteps, len(X_train_scaled)):\n",
    "            X_seq.append(X_train_scaled[i - timesteps:i])\n",
    "            y_seq.append(y_train_scaled[i])\n",
    "        X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X_seq.shape[1], X_seq.shape[2])))\n",
    "        for i in range(num_layers):\n",
    "            return_seq = i < num_layers - 1\n",
    "            model.add(LSTM(units=units, activation=activation, return_sequences=return_seq))\n",
    "            if dropout > 0:\n",
    "                model.add(Dropout(dropout))\n",
    "        model.add(Dense(1))\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        model.fit(X_seq, y_seq, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Previsão: a partir da última sequência do treino\n",
    "        last_input = df_train[feature_cols].iloc[-timesteps:].values\n",
    "        last_input_scaled = scaler_X.transform(last_input).reshape((1, timesteps, len(feature_cols)))\n",
    "\n",
    "        preds = []\n",
    "        input_seq = last_input_scaled.copy()\n",
    "\n",
    "        for _ in range(forecast_horizon):\n",
    "            pred_scaled = model.predict(input_seq, verbose=0)[0][0]\n",
    "            pred = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
    "            preds.append(pred)\n",
    "\n",
    "            # Roll the window\n",
    "            new_input = np.append(input_seq[:, 1:, :], [[[pred_scaled] * len(feature_cols)]], axis=1)\n",
    "            input_seq = new_input\n",
    "\n",
    "        y_test = df_test['price actual'].values[:forecast_horizon]\n",
    "        aligned_len = min(len(preds), len(y_test))\n",
    "\n",
    "        if aligned_len == 0:\n",
    "            print(\"⚠️ Previsão ignorada (sem alinhamento válido)\")\n",
    "            current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "            step += 1\n",
    "            continue\n",
    "\n",
    "        mae = mean_absolute_error(y_test[:aligned_len], preds[:aligned_len])\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[:aligned_len], preds[:aligned_len]))\n",
    "        mape = mean_absolute_percentage_error(y_test[:aligned_len], preds[:aligned_len]) * 100\n",
    "        rmae = mae / np.mean(np.abs(y_test[:aligned_len]))\n",
    "\n",
    "        results.append({\n",
    "            'start': current_test_start,\n",
    "            'end': current_test_end,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'rMAE': rmae\n",
    "        })\n",
    "\n",
    "        current_test_start += pd.Timedelta(hours=forecast_horizon)\n",
    "        step += 1\n",
    "\n",
    "    if not results:\n",
    "        print(\"❌ Nenhuma previsão válida foi registada.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03c9659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 [1] Janela: 2017-01-01 → 2017-01-07 | Train: 17544, Test: 168\n",
      "⚠️ Janela ignorada (dados insuficientes)\n",
      "📅 [2] Janela: 2017-01-08 → 2017-01-14 | Train: 17712, Test: 168\n",
      "⚠️ Janela ignorada (dados insuficientes)\n",
      "⏳ Progresso: 25% da janela de teste\n",
      "📅 [3] Janela: 2017-01-15 → 2017-01-21 | Train: 17880, Test: 168\n",
      "⚠️ Janela ignorada (dados insuficientes)\n",
      "📅 [4] Janela: 2017-01-22 → 2017-01-28 | Train: 18048, Test: 168\n",
      "⚠️ Janela ignorada (dados insuficientes)\n",
      "⏳ Progresso: 50% da janela de teste\n",
      "📅 [5] Janela: 2017-01-29 → 2017-02-04 | Train: 18216, Test: 168\n",
      "⚠️ Janela ignorada (dados insuficientes)\n",
      "📅 [6] Janela: 2017-02-05 → 2017-02-11 | Train: 18384, Test: 168\n",
      "⚠️ Janela ignorada (dados insuficientes)\n",
      "⏳ Progresso: 75% da janela de teste\n",
      "📅 [7] Janela: 2017-02-12 → 2017-02-18 | Train: 18552, Test: 168\n",
      "⚠️ Janela ignorada (dados insuficientes)\n",
      "📅 [8] Janela: 2017-02-19 → 2017-02-25 | Train: 18720, Test: 168\n",
      "⚠️ Janela ignorada (dados insuficientes)\n",
      "❌ Nenhuma previsão válida foi registada.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['MAE', 'RMSE', 'MAPE (%)', 'rMAE'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m\n\u001b[0;32m      1\u001b[0m lstm_results_tunning_window2 \u001b[38;5;241m=\u001b[39m longterm_sliding_expanding_lstm_tuning(\n\u001b[0;32m      2\u001b[0m     df,\n\u001b[0;32m      3\u001b[0m     train_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2015-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlstm_results_tunning_window2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRMSE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAPE (\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrMAE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['MAE', 'RMSE', 'MAPE (%)', 'rMAE'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "lstm_results_tunning_window2 = longterm_sliding_expanding_lstm_tuning(\n",
    "    df,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-02-28',\n",
    "    forecast_horizon=168,\n",
    "    timesteps=168,\n",
    "    units=50,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    activation='tanh',\n",
    "    learning_rate=0.001,\n",
    "    dropout=0.2,\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "print(lstm_results_tunning_window2[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b7d0f",
   "metadata": {},
   "source": [
    "# SARIMA TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6f0865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 91\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrMAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m: mae,\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: rmse,\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAPE (\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m: mape,\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrMAE\u001b[39m\u001b[38;5;124m'\u001b[39m: rmae\n\u001b[0;32m     89\u001b[0m     }])\n\u001b[1;32m---> 91\u001b[0m result_sarima \u001b[38;5;241m=\u001b[39m \u001b[43msarima_longterm_forecast_tuning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_prepared\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2015-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2016-12-31\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2017-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2017-01-15\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ou o intervalo que quiseres\u001b[39;49;00m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# semanal\u001b[39;49;00m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\n\u001b[0;32m    101\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 53\u001b[0m, in \u001b[0;36msarima_longterm_forecast_tuning\u001b[1;34m(df, train_start, train_end, test_start, test_end, forecast_horizon, order, seasonal_order, step_size_days)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     model \u001b[38;5;241m=\u001b[39m SARIMAX(\n\u001b[0;32m     46\u001b[0m         df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice actual\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     47\u001b[0m         order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m         simple_differencing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     )\n\u001b[1;32m---> 53\u001b[0m     model_fit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     forecast \u001b[38;5;241m=\u001b[39m model_fit\u001b[38;5;241m.\u001b[39mforecast(steps\u001b[38;5;241m=\u001b[39mforecast_horizon)\n\u001b[0;32m     55\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice actual\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:729\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth\n\u001b[1;32m--> 729\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlefit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincludes_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m           \u001b[49m\u001b[43mcov_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov_kwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    732\u001b[0m res\u001b[38;5;241m.\u001b[39mmlefit \u001b[38;5;241m=\u001b[39m mlefit\n\u001b[0;32m    733\u001b[0m res\u001b[38;5;241m.\u001b[39mmle_retvals \u001b[38;5;241m=\u001b[39m mlefit\u001b[38;5;241m.\u001b[39mmle_retvals\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:887\u001b[0m, in \u001b[0;36mMLEModel.smooth\u001b[1;34m(self, params, transformed, includes_fixed, complex_step, cov_type, cov_kwds, return_ssm, results_class, results_wrapper_class, **kwargs)\u001b[0m\n\u001b[0;32m    884\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minversion_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m INVERT_UNIVARIATE \u001b[38;5;241m|\u001b[39m SOLVE_LU\n\u001b[0;32m    886\u001b[0m \u001b[38;5;66;03m# Get the state space output\u001b[39;00m\n\u001b[1;32m--> 887\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm\u001b[38;5;241m.\u001b[39msmooth(complex_step\u001b[38;5;241m=\u001b[39mcomplex_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;66;03m# Wrap in a results object\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_results(params, result, return_ssm, cov_type,\n\u001b[0;32m    891\u001b[0m                           cov_kwds, results_class,\n\u001b[0;32m    892\u001b[0m                           results_wrapper_class)\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_smoother.py:423\u001b[0m, in \u001b[0;36mKalmanSmoother.smooth\u001b[1;34m(self, smoother_output, smooth_method, results, run_filter, prefix, complex_step, update_representation, update_filter, update_smoother, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# Update the results\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_smoother:\n\u001b[1;32m--> 423\u001b[0m     \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_smoother\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmoother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_smoother.py:645\u001b[0m, in \u001b[0;36mSmootherResults.update_smoother\u001b[1;34m(self, smoother)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmoother_disturbance_cov:\n\u001b[0;32m    640\u001b[0m     attributes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmoothed_measurement_disturbance_cov\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmoothed_state_disturbance_cov\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    643\u001b[0m     ]\n\u001b[1;32m--> 645\u001b[0m has_missing \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnmissing\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smoother_attributes:\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmoother_output\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:2485\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2482\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2486\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[0;32m   2488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "def sarima_longterm_forecast_tuning(\n",
    "    df,\n",
    "    train_start,\n",
    "    train_end,\n",
    "    test_start,\n",
    "    test_end,\n",
    "    forecast_horizon=168,\n",
    "    order=(1, 1, 1),\n",
    "    seasonal_order=(1, 1, 1, 168),\n",
    "    step_size_days=7\n",
    "):\n",
    "    df = df.copy()\n",
    "    df.index = df.index.tz_localize(None)\n",
    "\n",
    "    current_test_start = pd.to_datetime(test_start)\n",
    "    final_test_end = pd.to_datetime(test_end)\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    steps = (final_test_end - current_test_start).days + 1\n",
    "    checkpoints = {int(steps * 0.25): \"25%\", int(steps * 0.5): \"50%\", int(steps * 0.75): \"75%\"}\n",
    "\n",
    "    step = 0\n",
    "    while current_test_start + pd.Timedelta(hours=forecast_horizon - 1) <= final_test_end:\n",
    "        if step in checkpoints:\n",
    "            print(f\"⏳ Progresso: {checkpoints[step]} da janela de teste\")\n",
    "\n",
    "        current_test_end = current_test_start + pd.Timedelta(hours=forecast_horizon - 1)\n",
    "\n",
    "        df_train = df.loc[train_start:current_test_start - pd.Timedelta(hours=1)]\n",
    "        df_test = df.loc[current_test_start:current_test_end]\n",
    "\n",
    "        if len(df_test) < forecast_horizon:\n",
    "            current_test_start += pd.Timedelta(days=step_size_days)\n",
    "            step += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            model = SARIMAX(\n",
    "                df_train['price actual'],\n",
    "                order=order,\n",
    "                seasonal_order=seasonal_order,\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False,\n",
    "                simple_differencing=True\n",
    "            )\n",
    "            model_fit = model.fit(disp=False, maxiter=50)\n",
    "            forecast = model_fit.forecast(steps=forecast_horizon)\n",
    "            y_true = df_test['price actual'].values\n",
    "            y_pred = forecast.values\n",
    "\n",
    "            all_y_true.extend(y_true)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "            print(f\"✅ Passo {step + 1} concluído: {current_test_start.date()} → {current_test_end.date()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erro no passo {step + 1}: {e}\")\n",
    "\n",
    "        current_test_start += pd.Timedelta(days=step_size_days)\n",
    "        step += 1\n",
    "\n",
    "    if len(all_y_true) == 0:\n",
    "        print(\"⚠️ Nenhuma previsão foi feita.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mae = mean_absolute_error(all_y_true, all_y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(all_y_true, all_y_pred))\n",
    "    mape = mean_absolute_percentage_error(all_y_true, all_y_pred) * 100\n",
    "    rmae = mae / np.mean(np.abs(all_y_true))\n",
    "\n",
    "    print(\"\\n📊 Métricas finais SARIMA:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE (%): {mape:.4f}\")\n",
    "    print(f\"rMAE: {rmae:.4f}\")\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE (%)': mape,\n",
    "        'rMAE': rmae\n",
    "    }])\n",
    "\n",
    "result_sarima = sarima_longterm_forecast_tuning(\n",
    "    df=df_prepared,\n",
    "    train_start='2015-01-01',\n",
    "    train_end='2016-12-31',\n",
    "    test_start='2017-01-01',\n",
    "    test_end='2017-01-15',  # ou o intervalo que quiseres\n",
    "    forecast_horizon=168,\n",
    "    order=(1, 1, 1),\n",
    "    seasonal_order=(1, 1, 1, 168),  # semanal\n",
    "    step_size_days=7\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89648fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\gpere\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "sarima_result_tuning = sliding_expanding_longterm_sarima_tuning(\n",
    "        df=df_prepared,\n",
    "        train_start='2015-01-01',\n",
    "        train_end='2016-12-31',\n",
    "        test_start='2017-01-01',\n",
    "        test_end='2017-01-15',\n",
    "        forecast_horizon=168,\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(1, 1, 1, 168),\n",
    "        step_size_days=7\n",
    "    )\n",
    "\n",
    "print(sarima_result_tuning[['MAE', 'RMSE', 'MAPE (%)', 'rMAE']].mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099fbe3",
   "metadata": {},
   "source": [
    "# ARIMA 7 Day Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "\n",
    "# Definir intervalo\n",
    "train_start = '2016-01-01'\n",
    "train_end = '2016-12-31'\n",
    "test_start = '2017-01-01'\n",
    "test_end = '2017-01-07 23:00:00'  # primeiros 7 dias\n",
    "\n",
    "# Preparar dados\n",
    "df_train = df.loc[train_start:train_end]\n",
    "df_test = df.loc[test_start:test_end]\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in ['price actual', 'price day ahead']]\n",
    "\n",
    "# ARIMA\n",
    "arima_model = ARIMA(df_train['price actual'], order=(2, 1, 2)).fit()\n",
    "arima_forecast = arima_model.forecast(steps=168)\n",
    "\n",
    "# GLM\n",
    "X_train = df_train[feature_cols].values\n",
    "X_test = df_test[feature_cols].values\n",
    "y_test = df_test['price actual'].values\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "glm = LinearRegression()\n",
    "glm.fit(X_train_scaled, df_train['price actual'].values)\n",
    "glm_forecast = glm.predict(X_test_scaled)\n",
    "\n",
    "# Combinação\n",
    "y_pred = (arima_forecast.values[:168] + glm_forecast[:168]) / 2\n",
    "\n",
    "# Métricas\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "rmae = mae / np.mean(np.abs(y_test))\n",
    "\n",
    "# Gráfico\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_test.index, y_test, label='Actual Price', color='black', linewidth=2)\n",
    "plt.plot(df_test.index, y_pred, label='Forecasted Price (ARIMA-GLM)', color='blue', linestyle='--')\n",
    "plt.title('ARIMA-GLM Forecast vs Actual – Jan 1–7, 2017')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Electricity Price (€/MWh)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Inserir métricas no gráfico\n",
    "metrics_text = (\n",
    "    f\"MAE:  {mae:.2f} €/MWh\\n\"\n",
    "    f\"RMSE: {rmse:.2f} €/MWh\\n\"\n",
    "    f\"MAPE: {mape:.2f} %\\n\"\n",
    "    f\"rMAE: {rmae:.4f}\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"MAE:  {mae:.2f} €/MWh\")\n",
    "print(f\"RMSE: {rmse:.2f} €/MWh\")\n",
    "print(f\"MAPE: {mape:.2f} %\")\n",
    "print(f\"rMAE: {rmae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d402b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# Definir período de treino e teste\n",
    "train_start = '2016-01-01'\n",
    "train_end = '2016-12-31'\n",
    "test_start = '2017-01-01'\n",
    "test_end = '2017-01-07 23:00:00'  # primeiros 7 dias\n",
    "\n",
    "# Preparar dados\n",
    "df_train = df.loc[train_start:train_end]\n",
    "df_test = df.loc[test_start:test_end]\n",
    "\n",
    "drop_cols = ['price actual', 'price day ahead']\n",
    "feature_cols = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "X_train = df_train[feature_cols].values\n",
    "y_train = df_train['price actual'].values\n",
    "X_test = df_test[feature_cols].values\n",
    "y_test = df_test['price actual'].values\n",
    "\n",
    "# Normalizar\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Instanciar e treinar modelo com melhores parâmetros\n",
    "model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Previsão\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Métricas\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "rmae = mae / np.mean(np.abs(y_test))\n",
    "\n",
    "# Gráfico\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_test.index, y_test, label='Actual Price', color='black', linewidth=2)\n",
    "plt.plot(df_test.index, y_pred, label='Forecasted Price (XGBoost)', color='orange', linestyle='--')\n",
    "plt.title('XGBoost Forecast vs Actual – Jan 1–7, 2017')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Electricity Price (€/MWh)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar métricas na consola\n",
    "print(f\"MAE:  {mae:.2f} €/MWh\")\n",
    "print(f\"RMSE: {rmse:.2f} €/MWh\")\n",
    "print(f\"MAPE: {mape:.2f} %\")\n",
    "print(f\"rMAE: {rmae:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
